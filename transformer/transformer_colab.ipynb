{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer をゼロから実装\n",
    "\n",
    "Google Colab で GPU を使って学習します。\n",
    "\n",
    "**GPU を有効にする:** ランタイム → ランタイムのタイプを変更 → GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU確認\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self-Attention の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    \n",
    "    Q (Query):  「何を探しているか」\n",
    "    K (Key):    「何を持っているか」\n",
    "    V (Value):  「実際の情報」\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / √d) × V\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置エンコーディング: 順番情報を追加\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed Forward Network\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, hidden_size: int = None):\n",
    "        super().__init__()\n",
    "        hidden_size = hidden_size or embed_size * 4\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, embed_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Attention → Add & Norm → FeedForward → Add & Norm\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = FeedForward(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"文章生成用 Transformer (GPT風)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_size: int = 128,\n",
    "        num_heads: int = 4,\n",
    "        num_layers: int = 4,\n",
    "        max_len: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(embed_size)\n",
    "        self.output = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.token_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.ln_final(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int, device):\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text: str, seq_length: int = 128):\n",
    "        self.seq_length = seq_length\n",
    "        self.chars = sorted(set(text))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.data = torch.tensor([self.char_to_idx[ch] for ch in text])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.seq_length]\n",
    "        y = self.data[idx + 1 : idx + self.seq_length + 1]\n",
    "        return x, y\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.char_to_idx[ch] for ch in text])\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return \"\".join(self.idx_to_char[i.item()] for i in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習データの準備\n",
    "\n",
    "シェイクスピアのテキストをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# シェイクスピア全集をダウンロード\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urllib.request.urlretrieve(url, \"shakespeare.txt\")\n",
    "\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"テキスト長: {len(text):,} 文字\")\n",
    "print(\"\\n--- 冒頭 ---\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 128\n",
    "EMBED_SIZE = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "EPOCHS = 10\n",
    "LR = 3e-4\n",
    "\n",
    "# デバイス\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"デバイス: {device}\")\n",
    "\n",
    "# データ\n",
    "dataset = TextDataset(text, SEQ_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"語彙数: {dataset.vocab_size}\")\n",
    "print(f\"バッチ数: {len(dataloader)}/エポック\")\n",
    "\n",
    "# モデル\n",
    "model = Transformer(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"パラメータ数: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ループ\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        mask = Transformer.create_causal_mask(x.size(1), device)\n",
    "\n",
    "        logits = model(x, mask)\n",
    "        loss = criterion(logits.view(-1, dataset.vocab_size), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"\\rEpoch {epoch+1}: {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\", end=\"\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"\\rEpoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 文章生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, dataset, start_text=\"The \", length=500, temperature=0.8):\n",
    "    model.eval()\n",
    "    tokens = dataset.encode(start_text).unsqueeze(0).to(device)\n",
    "    generated = start_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            mask = Transformer.create_causal_mask(tokens.size(1), device)\n",
    "            logits = model(tokens, mask)\n",
    "            next_logits = logits[0, -1, :] / temperature\n",
    "            probs = torch.softmax(next_logits, dim=0)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            generated += dataset.idx_to_char[next_token.item()]\n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "            if tokens.size(1) > 512:\n",
    "                tokens = tokens[:, -512:]\n",
    "\n",
    "    return generated\n",
    "\n",
    "\n",
    "# 生成テスト\n",
    "print(generate(model, dataset, \"First Citizen:\", length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 別の開始文字で試す\n",
    "print(generate(model, dataset, \"ROMEO:\", length=500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
